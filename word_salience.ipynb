{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "from utils.text_analysis_transformers import RemovePunctuation, RemoveNonAscii\n",
    "from utils.text_analysis_transformers import NltkWordTokenizer, WordLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from utils.TimeBasedCV import TimeBasedCV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from utils.data_utils import import_reddit_set, import_uk_confidence\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit word count analysis\n",
    "\n",
    "Aiming to create the table;\n",
    "\n",
    "|             | Avg Intensity (weighted by the salience of the words in relation to the outcome variable) | Frequency of Words | Customer Confidence | Saving Ratio |\n",
    "|-------------|---------------------------------------------------------------------------------------------------|----------------------------|-----------------------------|----------------------|\n",
    "| Correlation |                                                                                                   |                            |                             |                      |\n",
    "| 1 Month     |                                                                                                   |                            |                             |                      |\n",
    "| 3 Month     |                                                                                                   |                            |                             |                      |\n",
    "| 6 Month     |                                                                                                   |                            |                             |                      |\n",
    "| 9 Months    |                                                                                                   |                            |                             |                      |\n",
    "\n",
    "## Outputs;\n",
    "Does each feature correlate with the target variables?\n",
    "Do the targets correlate with each other? (sanity check)\n",
    "Do the features have predictive power? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(location=\"US\", project=\"goldenfleece\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM final_task.alltones\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    "    project='goldenfleece'\n",
    ")  # API request - starts the query\n",
    "\n",
    "all_tones_words = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>axe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words\n",
       "0    ha\n",
       "1    no\n",
       "2   ass\n",
       "3   axe\n",
       "4   bad"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tones_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exposure       2\n",
       "prisoners      2\n",
       "probability    2\n",
       "deadweight     2\n",
       "contracting    2\n",
       "              ..\n",
       "yardstick      1\n",
       "ticker         1\n",
       "foreclosure    1\n",
       "escapes        1\n",
       "adopts         1\n",
       "Name: words, Length: 7552, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_tones_words['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting - not all values are unique. will drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>axe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words\n",
       "0    ha\n",
       "1    no\n",
       "2   ass\n",
       "3   axe\n",
       "4   bad"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = all_tones_words.drop_duplicates('words', ignore_index=True)\n",
    "\n",
    "unique_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power           1\n",
       "unbiased        1\n",
       "manipulated     1\n",
       "appeals         1\n",
       "cruelty         1\n",
       "               ..\n",
       "remanded        1\n",
       "legislator      1\n",
       "defamations     1\n",
       "invalidating    1\n",
       "adopts          1\n",
       "Name: words, Length: 7552, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(unique_words['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words.to_csv('data/vocabulary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ha',\n",
       " 1: 'no',\n",
       " 2: 'ass',\n",
       " 3: 'axe',\n",
       " 4: 'bad',\n",
       " 5: 'ban',\n",
       " 6: 'big',\n",
       " 7: 'cry',\n",
       " 8: 'cut',\n",
       " 9: 'die',\n",
       " 10: 'dud',\n",
       " 11: 'fad',\n",
       " 12: 'fag',\n",
       " 13: 'fan',\n",
       " 14: 'fit',\n",
       " 15: 'flu',\n",
       " 16: 'ftw',\n",
       " 17: 'fud',\n",
       " 18: 'fun',\n",
       " 19: 'gag',\n",
       " 20: 'god',\n",
       " 21: 'gun',\n",
       " 22: 'hid',\n",
       " 23: 'hug',\n",
       " 24: 'ill',\n",
       " 25: 'joy',\n",
       " 26: 'lag',\n",
       " 27: 'law',\n",
       " 28: 'lie',\n",
       " 29: 'lol',\n",
       " 30: 'mad',\n",
       " 31: 'may',\n",
       " 32: 'odd',\n",
       " 33: 'oks',\n",
       " 34: 'pay',\n",
       " 35: 'rig',\n",
       " 36: 'rob',\n",
       " 37: 'sad',\n",
       " 38: 'shy',\n",
       " 39: 'sue',\n",
       " 40: 'top',\n",
       " 41: 'war',\n",
       " 42: 'win',\n",
       " 43: 'won',\n",
       " 44: 'woo',\n",
       " 45: 'wow',\n",
       " 46: 'wtf',\n",
       " 47: 'yes',\n",
       " 48: '144a',\n",
       " 49: '1933',\n",
       " 50: '1986',\n",
       " 51: '2002',\n",
       " 52: 'able',\n",
       " 53: 'ache',\n",
       " 54: 'acid',\n",
       " 55: 'afdb',\n",
       " 56: 'agog',\n",
       " 57: 'alas',\n",
       " 58: 'ante',\n",
       " 59: 'anti',\n",
       " 60: 'anxi',\n",
       " 61: 'arab',\n",
       " 62: 'area',\n",
       " 63: 'arms',\n",
       " 64: 'aval',\n",
       " 65: 'avid',\n",
       " 66: 'axed',\n",
       " 67: 'back',\n",
       " 68: 'bail',\n",
       " 69: 'balk',\n",
       " 70: 'bank',\n",
       " 71: 'bans',\n",
       " 72: 'base',\n",
       " 73: 'bear',\n",
       " 74: 'best',\n",
       " 75: 'beta',\n",
       " 76: 'bias',\n",
       " 77: 'bill',\n",
       " 78: 'blah',\n",
       " 79: 'bloc',\n",
       " 80: 'blue',\n",
       " 81: 'bold',\n",
       " 82: 'bomb',\n",
       " 83: 'bona',\n",
       " 84: 'bond',\n",
       " 85: 'book',\n",
       " 86: 'boom',\n",
       " 87: 'bore',\n",
       " 88: 'bowl',\n",
       " 89: 'bull',\n",
       " 90: 'bust',\n",
       " 91: 'call',\n",
       " 92: 'calm',\n",
       " 93: 'capm',\n",
       " 94: 'care',\n",
       " 95: 'case',\n",
       " 96: 'cash',\n",
       " 97: 'cccn',\n",
       " 98: 'chic',\n",
       " 99: 'club',\n",
       " 100: 'cock',\n",
       " 101: 'code',\n",
       " 102: 'cone',\n",
       " 103: 'cool',\n",
       " 104: 'core',\n",
       " 105: 'cost',\n",
       " 106: 'crap',\n",
       " 107: 'crts',\n",
       " 108: 'csos',\n",
       " 109: 'cunt',\n",
       " 110: 'cute',\n",
       " 111: 'cuts',\n",
       " 112: 'damn',\n",
       " 113: 'data',\n",
       " 114: 'date',\n",
       " 115: 'dead',\n",
       " 116: 'dear',\n",
       " 117: 'debt',\n",
       " 118: 'deep',\n",
       " 119: 'deny',\n",
       " 120: 'desk',\n",
       " 121: 'dfid',\n",
       " 122: 'dick',\n",
       " 123: 'died',\n",
       " 124: 'dire',\n",
       " 125: 'dirt',\n",
       " 126: 'disc',\n",
       " 127: 'dock',\n",
       " 128: 'doom',\n",
       " 129: 'down',\n",
       " 130: 'drag',\n",
       " 131: 'drip',\n",
       " 132: 'drop',\n",
       " 133: 'drug',\n",
       " 134: 'dual',\n",
       " 135: 'dull',\n",
       " 136: 'duly',\n",
       " 137: 'dumb',\n",
       " 138: 'dump',\n",
       " 139: 'dupe',\n",
       " 140: 'duty',\n",
       " 141: 'eafe',\n",
       " 142: 'ease',\n",
       " 143: 'east',\n",
       " 144: 'easy',\n",
       " 145: 'ebit',\n",
       " 146: 'ebrd',\n",
       " 147: 'edge',\n",
       " 148: 'eery',\n",
       " 149: 'envy',\n",
       " 150: 'errs',\n",
       " 151: 'euro',\n",
       " 152: 'even',\n",
       " 153: 'evil',\n",
       " 154: 'exit',\n",
       " 155: 'face',\n",
       " 156: 'fact',\n",
       " 157: 'fail',\n",
       " 158: 'fair',\n",
       " 159: 'fake',\n",
       " 160: 'fame',\n",
       " 161: 'fasb',\n",
       " 162: 'fast',\n",
       " 163: 'fear',\n",
       " 164: 'fiat',\n",
       " 165: 'fide',\n",
       " 166: 'fine',\n",
       " 167: 'fire',\n",
       " 168: 'firm',\n",
       " 169: 'flaw',\n",
       " 170: 'flop',\n",
       " 171: 'flow',\n",
       " 172: 'fogs',\n",
       " 173: 'fond',\n",
       " 174: 'food',\n",
       " 175: 'fool',\n",
       " 176: 'form',\n",
       " 177: 'four',\n",
       " 178: 'free',\n",
       " 179: 'fret',\n",
       " 180: 'from',\n",
       " 181: 'ftaa',\n",
       " 182: 'fuck',\n",
       " 183: 'full',\n",
       " 184: 'fund',\n",
       " 185: 'gaap',\n",
       " 186: 'gain',\n",
       " 187: 'game',\n",
       " 188: 'gatt',\n",
       " 189: 'gift',\n",
       " 190: 'gini',\n",
       " 191: 'glad',\n",
       " 192: 'glee',\n",
       " 193: 'glum',\n",
       " 194: 'gold',\n",
       " 195: 'good',\n",
       " 196: 'gray',\n",
       " 197: 'grew',\n",
       " 198: 'grey',\n",
       " 199: 'grow',\n",
       " 200: 'haha',\n",
       " 201: 'hail',\n",
       " 202: 'halt',\n",
       " 203: 'hard',\n",
       " 204: 'harm',\n",
       " 205: 'hate',\n",
       " 206: 'hell',\n",
       " 207: 'help',\n",
       " 208: 'hero',\n",
       " 209: 'hide',\n",
       " 210: 'high',\n",
       " 211: 'hipc',\n",
       " 212: 'hoax',\n",
       " 213: 'home',\n",
       " 214: 'hope',\n",
       " 215: 'host',\n",
       " 216: 'huge',\n",
       " 217: 'hugs',\n",
       " 218: 'hurt',\n",
       " 219: 'iadb',\n",
       " 220: 'iapm',\n",
       " 221: 'idle',\n",
       " 222: 'item',\n",
       " 223: 'jerk',\n",
       " 224: 'join',\n",
       " 225: 'joke',\n",
       " 226: 'junk',\n",
       " 227: 'jury',\n",
       " 228: 'just',\n",
       " 229: 'keen',\n",
       " 230: 'kemp',\n",
       " 231: 'kill',\n",
       " 232: 'kind',\n",
       " 233: 'kiss',\n",
       " 234: 'lack',\n",
       " 235: 'lags',\n",
       " 236: 'lame',\n",
       " 237: 'land',\n",
       " 238: 'last',\n",
       " 239: 'late',\n",
       " 240: 'lawl',\n",
       " 241: 'laws',\n",
       " 242: 'lazy',\n",
       " 243: 'ldcs',\n",
       " 244: 'lead',\n",
       " 245: 'leak',\n",
       " 246: 'lean',\n",
       " 247: 'less',\n",
       " 248: 'levy',\n",
       " 249: 'liar',\n",
       " 250: 'lied',\n",
       " 251: 'lien',\n",
       " 252: 'life',\n",
       " 253: 'like',\n",
       " 254: 'line',\n",
       " 255: 'link',\n",
       " 256: 'list',\n",
       " 257: 'lmao',\n",
       " 258: 'loan',\n",
       " 259: 'lock',\n",
       " 260: 'long',\n",
       " 261: 'look',\n",
       " 262: 'loom',\n",
       " 263: 'lose',\n",
       " 264: 'loss',\n",
       " 265: 'lost',\n",
       " 266: 'love',\n",
       " 267: 'luck',\n",
       " 268: 'lump',\n",
       " 269: 'lurk',\n",
       " 270: 'made',\n",
       " 271: 'mean',\n",
       " 272: 'meas',\n",
       " 273: 'mess',\n",
       " 274: 'mill',\n",
       " 275: 'miss',\n",
       " 276: 'mmfs',\n",
       " 277: 'moan',\n",
       " 278: 'mock',\n",
       " 279: 'mode',\n",
       " 280: 'mope',\n",
       " 281: 'more',\n",
       " 282: 'most',\n",
       " 283: 'must',\n",
       " 284: 'myth',\n",
       " 285: 'n00b',\n",
       " 286: 'nash',\n",
       " 287: 'ness',\n",
       " 288: 'news',\n",
       " 289: 'ngos',\n",
       " 290: 'nice',\n",
       " 291: 'nolo',\n",
       " 292: 'noob',\n",
       " 293: 'note',\n",
       " 294: 'novo',\n",
       " 295: 'numb',\n",
       " 296: 'nuts',\n",
       " 297: 'oecd',\n",
       " 298: 'ofcs',\n",
       " 299: 'omit',\n",
       " 300: 'opec',\n",
       " 301: 'open',\n",
       " 302: 'opic',\n",
       " 303: 'over',\n",
       " 304: 'paid',\n",
       " 305: 'pain',\n",
       " 306: 'para',\n",
       " 307: 'pari',\n",
       " 308: 'pass',\n",
       " 309: 'path',\n",
       " 310: 'peak',\n",
       " 311: 'peso',\n",
       " 312: 'pill',\n",
       " 313: 'piss',\n",
       " 314: 'pity',\n",
       " 315: 'plan',\n",
       " 316: 'play',\n",
       " 317: 'plea',\n",
       " 318: 'pled',\n",
       " 319: 'plot',\n",
       " 320: 'plus',\n",
       " 321: 'poly',\n",
       " 322: 'poor',\n",
       " 323: 'port',\n",
       " 324: 'post',\n",
       " 325: 'pray',\n",
       " 326: 'pure',\n",
       " 327: 'quid',\n",
       " 328: 'quit',\n",
       " 329: 'race',\n",
       " 330: 'radr',\n",
       " 331: 'rage',\n",
       " 332: 'rant',\n",
       " 333: 'rape',\n",
       " 334: 'rash',\n",
       " 335: 'rata',\n",
       " 336: 'rate',\n",
       " 337: 'real',\n",
       " 338: 'rent',\n",
       " 339: 'rich',\n",
       " 340: 'riot',\n",
       " 341: 'risk',\n",
       " 342: 'robs',\n",
       " 343: 'rofl',\n",
       " 344: 'roll',\n",
       " 345: 'room',\n",
       " 346: 'ruin',\n",
       " 347: 'rule',\n",
       " 348: 'safe',\n",
       " 349: 'sale',\n",
       " 350: 'same',\n",
       " 351: 'save',\n",
       " 352: 'scam',\n",
       " 353: 'self',\n",
       " 354: 'sell',\n",
       " 355: 'semi',\n",
       " 356: 'sexy',\n",
       " 357: 'ship',\n",
       " 358: 'shit',\n",
       " 359: 'shut',\n",
       " 360: 'sick',\n",
       " 361: 'side',\n",
       " 362: 'sigh',\n",
       " 363: 'size',\n",
       " 364: 'slam',\n",
       " 365: 'slid',\n",
       " 366: 'slow',\n",
       " 367: 'slut',\n",
       " 368: 'smog',\n",
       " 369: 'snub',\n",
       " 370: 'soft',\n",
       " 371: 'sogo',\n",
       " 372: 'sold',\n",
       " 373: 'sole',\n",
       " 374: 'sore',\n",
       " 375: 'spam',\n",
       " 376: 'spin',\n",
       " 377: 'spot',\n",
       " 378: 'stab',\n",
       " 379: 'step',\n",
       " 380: 'stop',\n",
       " 381: 'suck',\n",
       " 382: 'sued',\n",
       " 383: 'sues',\n",
       " 384: 'sunk',\n",
       " 385: 'swan',\n",
       " 386: 'swap',\n",
       " 387: 'tard',\n",
       " 388: 'tare',\n",
       " 389: 'term',\n",
       " 390: 'test',\n",
       " 391: 'than',\n",
       " 392: 'thru',\n",
       " 393: 'tied',\n",
       " 394: 'tier',\n",
       " 395: 'time',\n",
       " 396: 'tits',\n",
       " 397: 'tops',\n",
       " 398: 'torn',\n",
       " 399: 'tort',\n",
       " 400: 'tout',\n",
       " 401: 'trap',\n",
       " 402: 'tree',\n",
       " 403: 'true',\n",
       " 404: 'twat',\n",
       " 405: 'twin',\n",
       " 406: 'ucom',\n",
       " 407: 'ugly',\n",
       " 408: 'unit',\n",
       " 409: 'unto',\n",
       " 410: 'used',\n",
       " 411: 'vary',\n",
       " 412: 'vent',\n",
       " 413: 'vile',\n",
       " 414: 'vote',\n",
       " 415: 'wacc',\n",
       " 416: 'wage',\n",
       " 417: 'walk',\n",
       " 418: 'wall',\n",
       " 419: 'want',\n",
       " 420: 'warm',\n",
       " 421: 'warn',\n",
       " 422: 'weak',\n",
       " 423: 'weep',\n",
       " 424: 'will',\n",
       " 425: 'wins',\n",
       " 426: 'wire',\n",
       " 427: 'wish',\n",
       " 428: 'with',\n",
       " 429: 'wooo',\n",
       " 430: 'woow',\n",
       " 431: 'work',\n",
       " 432: 'worn',\n",
       " 433: 'writ',\n",
       " 434: 'yeah',\n",
       " 435: 'year',\n",
       " 436: 'zero',\n",
       " 437: 'zone',\n",
       " 438: '2x2x2',\n",
       " 439: 'abhor',\n",
       " 440: 'abide',\n",
       " 441: 'abuse',\n",
       " 442: 'added',\n",
       " 443: 'adela',\n",
       " 444: 'admit',\n",
       " 445: 'adopt',\n",
       " 446: 'adore',\n",
       " 447: 'afesd',\n",
       " 448: 'after',\n",
       " 449: 'agent',\n",
       " 450: 'aging',\n",
       " 451: 'agree',\n",
       " 452: 'alarm',\n",
       " 453: 'alert',\n",
       " 454: 'alive',\n",
       " 455: 'allow',\n",
       " 456: 'alone',\n",
       " 457: 'amaze',\n",
       " 458: 'amend',\n",
       " 459: 'amuse',\n",
       " 460: 'anger',\n",
       " 461: 'angle',\n",
       " 462: 'angry',\n",
       " 463: 'annoy',\n",
       " 464: 'annul',\n",
       " 465: 'argue',\n",
       " 466: 'asian',\n",
       " 467: 'asset',\n",
       " 468: 'audit',\n",
       " 469: 'avert',\n",
       " 470: 'avoid',\n",
       " 471: 'await',\n",
       " 472: 'award',\n",
       " 473: 'awful',\n",
       " 474: 'backs',\n",
       " 475: 'badly',\n",
       " 476: 'baker',\n",
       " 477: 'banks',\n",
       " 478: 'based',\n",
       " 479: 'basic',\n",
       " 480: 'basis',\n",
       " 481: 'bills',\n",
       " 482: 'bitch',\n",
       " 483: 'black',\n",
       " 484: 'blame',\n",
       " 485: 'blank',\n",
       " 486: 'bless',\n",
       " 487: 'blind',\n",
       " 488: 'bliss',\n",
       " 489: 'block',\n",
       " 490: 'blush',\n",
       " 491: 'board',\n",
       " 492: 'bonds',\n",
       " 493: 'boost',\n",
       " 494: 'bored',\n",
       " 495: 'bound',\n",
       " 496: 'brady',\n",
       " 497: 'brain',\n",
       " 498: 'brave',\n",
       " 499: 'break',\n",
       " 500: 'bribe',\n",
       " 501: 'brisk',\n",
       " 502: 'broke',\n",
       " 503: 'brown',\n",
       " 504: 'bully',\n",
       " 505: 'buyer',\n",
       " 506: 'calms',\n",
       " 507: 'cares',\n",
       " 508: 'carve',\n",
       " 509: 'cease',\n",
       " 510: 'chain',\n",
       " 511: 'chaos',\n",
       " 512: 'charm',\n",
       " 513: 'chart',\n",
       " 514: 'cheat',\n",
       " 515: 'check',\n",
       " 516: 'cheer',\n",
       " 517: 'chips',\n",
       " 518: 'choke',\n",
       " 519: 'cites',\n",
       " 520: 'civil',\n",
       " 521: 'claim',\n",
       " 522: 'clash',\n",
       " 523: 'class',\n",
       " 524: 'clean',\n",
       " 525: 'clear',\n",
       " 526: 'coase',\n",
       " 527: 'cocky',\n",
       " 528: 'codes',\n",
       " 529: 'costs',\n",
       " 530: 'could',\n",
       " 531: 'count',\n",
       " 532: 'court',\n",
       " 533: 'cover',\n",
       " 534: 'cower',\n",
       " 535: 'cramp',\n",
       " 536: 'crash',\n",
       " 537: 'crazy',\n",
       " 538: 'cried',\n",
       " 539: 'cries',\n",
       " 540: 'crime',\n",
       " 541: 'crisi',\n",
       " 542: 'crony',\n",
       " 543: 'cross',\n",
       " 544: 'cruel',\n",
       " 545: 'crush',\n",
       " 546: 'cubic',\n",
       " 547: 'curse',\n",
       " 548: 'curve',\n",
       " 549: 'cycle',\n",
       " 550: 'cynic',\n",
       " 551: 'death',\n",
       " 552: 'debit',\n",
       " 553: 'defer',\n",
       " 554: 'delay',\n",
       " 555: 'delta',\n",
       " 556: 'deter',\n",
       " 557: 'dirty',\n",
       " 558: 'dizzy',\n",
       " 559: 'dodgy',\n",
       " 560: 'doing',\n",
       " 561: 'doubt',\n",
       " 562: 'draft',\n",
       " 563: 'drags',\n",
       " 564: 'drain',\n",
       " 565: 'dread',\n",
       " 566: 'dream',\n",
       " 567: 'drown',\n",
       " 568: 'drunk',\n",
       " 569: 'dummy',\n",
       " 570: 'dumps',\n",
       " 571: 'duped',\n",
       " 572: 'dutch',\n",
       " 573: 'eager',\n",
       " 574: 'early',\n",
       " 575: 'eerie',\n",
       " 576: 'empty',\n",
       " 577: 'enemy',\n",
       " 578: 'enjoy',\n",
       " 579: 'ennui',\n",
       " 580: 'entry',\n",
       " 581: 'erode',\n",
       " 582: 'erred',\n",
       " 583: 'error',\n",
       " 584: 'evade',\n",
       " 585: 'event',\n",
       " 586: 'evict',\n",
       " 587: 'expel',\n",
       " 588: 'extra',\n",
       " 589: 'facie',\n",
       " 590: 'facto',\n",
       " 591: 'fails',\n",
       " 592: 'faith',\n",
       " 593: 'fakes',\n",
       " 594: 'false',\n",
       " 595: 'farce',\n",
       " 596: 'fault',\n",
       " 597: 'favor',\n",
       " 598: 'fears',\n",
       " 599: 'field',\n",
       " 600: 'fight',\n",
       " 601: 'final',\n",
       " 602: 'fined',\n",
       " 603: 'fines',\n",
       " 604: 'fired',\n",
       " 605: 'first',\n",
       " 606: 'fixed',\n",
       " 607: 'flaws',\n",
       " 608: 'flees',\n",
       " 609: 'float',\n",
       " 610: 'floor',\n",
       " 611: 'flops',\n",
       " 612: 'flows',\n",
       " 613: 'fools',\n",
       " 614: 'force',\n",
       " 615: 'forma',\n",
       " 616: 'fraud',\n",
       " 617: 'fresh',\n",
       " 618: 'fuked',\n",
       " 619: 'funds',\n",
       " 620: 'funky',\n",
       " 621: 'funny',\n",
       " 622: 'gains',\n",
       " 623: 'gamma',\n",
       " 624: 'ghost',\n",
       " 625: 'giddy',\n",
       " 626: 'gloom',\n",
       " 627: 'glory',\n",
       " 628: 'going',\n",
       " 629: 'goods',\n",
       " 630: 'grace',\n",
       " 631: 'grand',\n",
       " 632: 'grant',\n",
       " 633: 'graph',\n",
       " 634: 'grave',\n",
       " 635: 'great',\n",
       " 636: 'greed',\n",
       " 637: 'green',\n",
       " 638: 'greet',\n",
       " 639: 'grief',\n",
       " 640: 'gross',\n",
       " 641: 'group',\n",
       " 642: 'guest',\n",
       " 643: 'guilt',\n",
       " 644: 'happy',\n",
       " 645: 'hardy',\n",
       " 646: 'harms',\n",
       " 647: 'harsh',\n",
       " 648: 'hated',\n",
       " 649: 'hates',\n",
       " 650: 'haunt',\n",
       " 651: 'haven',\n",
       " 652: 'havoc',\n",
       " 653: 'hedge',\n",
       " 654: 'helps',\n",
       " 655: 'hicks',\n",
       " 656: 'hides',\n",
       " 657: 'honor',\n",
       " 658: 'hopes',\n",
       " 659: 'house',\n",
       " 660: 'human',\n",
       " 661: 'humor',\n",
       " 662: 'hurts',\n",
       " 663: 'ideal',\n",
       " 664: 'idiot',\n",
       " 665: 'idled',\n",
       " 666: 'index',\n",
       " 667: 'input',\n",
       " 668: 'inter',\n",
       " 669: 'intra',\n",
       " 670: 'irate',\n",
       " 671: 'irony',\n",
       " 672: 'issue',\n",
       " 673: 'itchy',\n",
       " 674: 'items',\n",
       " 675: 'jesus',\n",
       " 676: 'jewel',\n",
       " 677: 'joint',\n",
       " 678: 'jokes',\n",
       " 679: 'jolly',\n",
       " 680: 'jones',\n",
       " 681: 'jumpy',\n",
       " 682: 'juris',\n",
       " 683: 'juror',\n",
       " 684: 'kills',\n",
       " 685: 'kudos',\n",
       " 686: 'labor',\n",
       " 687: 'lacks',\n",
       " 688: 'lapse',\n",
       " 689: 'large',\n",
       " 690: 'latin',\n",
       " 691: 'laugh',\n",
       " 692: 'lease',\n",
       " 693: 'least',\n",
       " 694: 'leave',\n",
       " 695: 'legal',\n",
       " 696: 'level',\n",
       " 697: 'liars',\n",
       " 698: 'libel',\n",
       " 699: 'libid',\n",
       " 700: 'libor',\n",
       " 701: 'light',\n",
       " 702: 'liked',\n",
       " 703: 'likes',\n",
       " 704: 'limit',\n",
       " 705: 'livid',\n",
       " 706: 'lmfao',\n",
       " 707: 'loans',\n",
       " 708: 'lobby',\n",
       " 709: 'local',\n",
       " 710: 'looms',\n",
       " 711: 'loose',\n",
       " 712: 'loser',\n",
       " 713: 'loses',\n",
       " 714: 'loved',\n",
       " 715: 'loyal',\n",
       " 716: 'lucky',\n",
       " 717: 'lurks',\n",
       " 718: 'lying',\n",
       " 719: 'macro',\n",
       " 720: 'madly',\n",
       " 721: 'maker',\n",
       " 722: 'maybe',\n",
       " 723: 'meade',\n",
       " 724: 'medal',\n",
       " 725: 'mercy',\n",
       " 726: 'merry',\n",
       " 727: 'micro',\n",
       " 728: 'might',\n",
       " 729: 'mirth',\n",
       " 730: 'mixed',\n",
       " 731: 'moans',\n",
       " 732: 'mocks',\n",
       " 733: 'model',\n",
       " 734: 'money',\n",
       " 735: 'moody',\n",
       " 736: 'moral',\n",
       " 737: 'moron',\n",
       " 738: 'mourn',\n",
       " 739: 'mover',\n",
       " 740: 'multi',\n",
       " 741: 'nafta',\n",
       " 742: 'naics',\n",
       " 743: 'nairu',\n",
       " 744: 'naive',\n",
       " 745: 'nakes',\n",
       " 746: 'nasty',\n",
       " 747: 'needy',\n",
       " 748: 'never',\n",
       " 749: 'newly',\n",
       " 750: 'nifty',\n",
       " 751: 'noble',\n",
       " 752: 'noisy',\n",
       " 753: 'nopat',\n",
       " 754: 'north',\n",
       " 755: 'nosey',\n",
       " 756: 'notes',\n",
       " 757: 'novel',\n",
       " 758: 'offer',\n",
       " 759: 'ohlin',\n",
       " 760: 'omits',\n",
       " 761: 'order',\n",
       " 762: 'owned',\n",
       " 763: 'oxley',\n",
       " 764: 'panic',\n",
       " 765: 'paper',\n",
       " 766: 'paris',\n",
       " 767: 'party',\n",
       " 768: 'passu',\n",
       " 769: 'pcaob',\n",
       " 770: 'peace',\n",
       " 771: 'pefco',\n",
       " 772: 'peril',\n",
       " 773: 'pesky',\n",
       " 774: 'petty',\n",
       " 775: 'pique',\n",
       " 776: 'place',\n",
       " 777: 'plaza',\n",
       " 778: 'plead',\n",
       " 779: 'pleas',\n",
       " 780: 'point',\n",
       " 781: 'poses',\n",
       " 782: 'power',\n",
       " 783: 'prays',\n",
       " 784: 'prblm',\n",
       " 785: 'price',\n",
       " 786: 'prick',\n",
       " 787: 'prima',\n",
       " 788: 'prime',\n",
       " 789: 'prone',\n",
       " 790: 'proud',\n",
       " 791: 'proxy',\n",
       " 792: 'pushy',\n",
       " 793: 'quasi',\n",
       " 794: 'quick',\n",
       " 795: 'quota',\n",
       " 796: 'quote',\n",
       " 797: 'rainy',\n",
       " 798: 'range',\n",
       " 799: 'rants',\n",
       " 800: 'rates',\n",
       " 801: 'ratio',\n",
       " 802: 'reach',\n",
       " 803: 'rebut',\n",
       " 804: 'rider',\n",
       " 805: 'right',\n",
       " 806: 'riots',\n",
       " 807: 'risks',\n",
       " 808: 'risky',\n",
       " 809: 'robed',\n",
       " 810: 'rotfl',\n",
       " 811: 'round',\n",
       " 812: 'ruins',\n",
       " 813: 'rules',\n",
       " 814: 'sadly',\n",
       " 815: 'sales',\n",
       " 816: 'sappy',\n",
       " 817: 'saved',\n",
       " 818: 'scale',\n",
       " 819: 'scams',\n",
       " 820: 'scare',\n",
       " 821: 'scary',\n",
       " 822: 'scold',\n",
       " 823: 'scoop',\n",
       " 824: 'scope',\n",
       " 825: 'score',\n",
       " 826: 'scorn',\n",
       " 827: 'seems',\n",
       " 828: 'seize',\n",
       " 829: 'sever',\n",
       " 830: 'shaky',\n",
       " 831: 'shall',\n",
       " 832: 'shame',\n",
       " 833: 'shape',\n",
       " 834: 'share',\n",
       " 835: 'shark',\n",
       " 836: 'sheet',\n",
       " 837: 'shelf',\n",
       " 838: 'shock',\n",
       " 839: 'shoot',\n",
       " 840: 'short',\n",
       " 841: 'shrew',\n",
       " 842: 'shuts',\n",
       " 843: 'sight',\n",
       " 844: 'silly',\n",
       " 845: 'skill',\n",
       " 846: 'slash',\n",
       " 847: 'slick',\n",
       " 848: 'slide',\n",
       " 849: 'slope',\n",
       " 850: 'slump',\n",
       " 851: 'small',\n",
       " 852: 'smart',\n",
       " 853: 'smear',\n",
       " 854: 'smile',\n",
       " 855: 'smoot',\n",
       " 856: 'snake',\n",
       " 857: 'snubs',\n",
       " 858: 'solid',\n",
       " 859: 'solow',\n",
       " 860: 'solve',\n",
       " 861: 'sorry',\n",
       " 862: 'sosha',\n",
       " 863: 'sound',\n",
       " 864: 'south',\n",
       " 865: 'space',\n",
       " 866: 'spark',\n",
       " 867: 'spend',\n",
       " 868: 'spill',\n",
       " 869: 'split',\n",
       " 870: 'spoke',\n",
       " 871: 'stabs',\n",
       " 872: 'stall',\n",
       " 873: 'stamp',\n",
       " 874: 'state',\n",
       " 875: 'steal',\n",
       " 876: 'stock',\n",
       " 877: 'stops',\n",
       " 878: 'stout',\n",
       " 879: 'stuck',\n",
       " 880: 'suave',\n",
       " 881: 'sucks',\n",
       " 882: 'suing',\n",
       " 883: 'sulky',\n",
       " 884: 'super',\n",
       " 885: 'swear',\n",
       " 886: 'sweet',\n",
       " 887: 'swift',\n",
       " 888: 'swiss',\n",
       " 889: 'table',\n",
       " 890: 'taint',\n",
       " 891: 'taker',\n",
       " 892: 'taxes',\n",
       " 893: 'tears',\n",
       " 894: 'tense',\n",
       " 895: 'tepid',\n",
       " 896: 'terms',\n",
       " 897: 'thank',\n",
       " 898: 'tiger',\n",
       " 899: 'timid',\n",
       " 900: 'tired',\n",
       " 901: 'tobin',\n",
       " 902: 'torts',\n",
       " 903: 'total',\n",
       " 904: 'tough',\n",
       " 905: 'touts',\n",
       " 906: 'track',\n",
       " 907: 'trade',\n",
       " 908: 'trees',\n",
       " 909: 'trend',\n",
       " 910: 'trust',\n",
       " 911: 'tumor',\n",
       " 912: 'under',\n",
       " 913: 'undue',\n",
       " 914: 'unfit',\n",
       " 915: 'union',\n",
       " 916: 'upset',\n",
       " 917: 'using',\n",
       " 918: 'usurp',\n",
       " 919: 'usury',\n",
       " 920: 'vague',\n",
       " 921: 'value',\n",
       " 922: 'vanek',\n",
       " 923: 'viner',\n",
       " 924: 'warns',\n",
       " 925: 'waste',\n",
       " 926: 'water',\n",
       " 927: 'weary',\n",
       " 928: 'weird',\n",
       " 929: 'white',\n",
       " 930: 'whore',\n",
       " 931: 'woods',\n",
       " 932: 'world',\n",
       " 933: 'worri',\n",
       " 934: 'worry',\n",
       " 935: 'worse',\n",
       " 936: 'worst',\n",
       " 937: 'worth',\n",
       " 938: 'wowow',\n",
       " 939: 'wowww',\n",
       " 940: 'wreck',\n",
       " 941: 'writs',\n",
       " 942: 'wrong',\n",
       " 943: 'yates',\n",
       " 944: 'yeees',\n",
       " 945: 'yield',\n",
       " 946: 'yucky',\n",
       " 947: 'yummy',\n",
       " 948: 'abhors',\n",
       " 949: 'aboard',\n",
       " 950: 'abrupt',\n",
       " 951: 'absorb',\n",
       " 952: 'abused',\n",
       " 953: 'abuses',\n",
       " 954: 'accept',\n",
       " 955: 'access',\n",
       " 956: 'accord',\n",
       " 957: 'accuse',\n",
       " 958: 'aching',\n",
       " 959: 'acquit',\n",
       " 960: 'action',\n",
       " 961: 'active',\n",
       " 962: 'admire',\n",
       " 963: 'admits',\n",
       " 964: 'adopts',\n",
       " 965: 'adored',\n",
       " 966: 'adores',\n",
       " 967: 'afraid',\n",
       " 968: 'agency',\n",
       " 969: 'aghast',\n",
       " 970: 'agreed',\n",
       " 971: 'agrees',\n",
       " 972: 'ailing',\n",
       " 973: 'allege',\n",
       " 974: 'almost',\n",
       " 975: 'always',\n",
       " 976: 'amazed',\n",
       " 977: 'amazes',\n",
       " 978: 'amends',\n",
       " 979: 'amused',\n",
       " 980: 'anchor',\n",
       " 981: 'angers',\n",
       " 982: 'annoys',\n",
       " 983: 'annual',\n",
       " 984: 'annuls',\n",
       " 985: 'apathy',\n",
       " 986: 'appeal',\n",
       " 987: 'appear',\n",
       " 988: 'ardent',\n",
       " 989: 'argued',\n",
       " 990: 'arrest',\n",
       " 991: 'ashame',\n",
       " 992: 'assets',\n",
       " 993: 'assume',\n",
       " 994: 'assure',\n",
       " 995: 'attack',\n",
       " 996: 'attain',\n",
       " 997: 'attest',\n",
       " 998: 'attorn',\n",
       " 999: 'averse',\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_dict = unique_words['words'].to_dict()\n",
    "\n",
    "unique_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping needs to be reversed for the sklearn countvectorizer!\n",
    "vocab = {v: k for k, v in unique_words_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py:1092: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>Exhibit A : Sir Christopher Cope : https://en....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>Am I supposed to feel bad for their choices or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>&amp;gt; which almost entirely seems to be an Isla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>the darkest timeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>A vote for Leave is a vote for Austerity 2: Br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      body\n",
       "date                                                      \n",
       "2019-02  Exhibit A : Sir Christopher Cope : https://en....\n",
       "2019-02  Am I supposed to feel bad for their choices or...\n",
       "2019-02  &gt; which almost entirely seems to be an Isla...\n",
       "2019-02                               the darkest timeline\n",
       "2019-02  A vote for Leave is a vote for Austerity 2: Br..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = import_reddit_set(rows=999999)\n",
    "reddit['date'] = reddit['date'].dt.to_period(\"M\")\n",
    "reddit.set_index('date', inplace=True)\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2019-02', '2019-03', '2019-12', '2019-06', '2019-08', '2019-11',\n",
       "             '2019-09', '2019-04', '2019-01', '2019-07', '2019-05', '2019-10'],\n",
       "            dtype='period[M]', name='date', freq='M')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(reddit.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_of_text = []\n",
    "index = []\n",
    "for month in pd.unique(reddit.index):\n",
    "\n",
    "    month_of_text = ''.join(reddit.loc[month, 'body'].values.tolist())\n",
    "\n",
    "    # remove digits..\n",
    "    month_of_text = ''.join(i for i in month_of_text if not i.isdigit())\n",
    "\n",
    "    months_of_text.append(month_of_text)\n",
    "    index.append(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_pipe = Pipeline([\n",
    "    ('remove_non_ascii', RemoveNonAscii()),\n",
    "    ('remove_punctuation', RemovePunctuation()),\n",
    "    ('lemmatize', WordLemmatizer()),\n",
    "    ('count_vec', CountVectorizer(stop_words='english',\n",
    "                                  lowercase=True,\n",
    "                                  ngram_range=(1, 3), # this is very memory expensive!\n",
    "                                  vocabulary= vocab)\n",
    "                                    )\n",
    "                                ])\n",
    "\n",
    "word_tfidf_pipe = Pipeline([\n",
    "    ('remove_non_ascii', RemoveNonAscii()),\n",
    "    ('remove_punctuation', RemovePunctuation()),\n",
    "    ('lemmatize', WordLemmatizer()),\n",
    "    ('count_vec', CountVectorizer(stop_words='english',\n",
    "                                  lowercase=True,\n",
    "                                  vocabulary= vocab)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "                                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ha</th>\n",
       "      <th>no</th>\n",
       "      <th>ass</th>\n",
       "      <th>axe</th>\n",
       "      <th>bad</th>\n",
       "      <th>ban</th>\n",
       "      <th>big</th>\n",
       "      <th>cry</th>\n",
       "      <th>cut</th>\n",
       "      <th>die</th>\n",
       "      <th>...</th>\n",
       "      <th>misclassifications</th>\n",
       "      <th>misinterpretations</th>\n",
       "      <th>misrepresentations</th>\n",
       "      <th>multifunctionality</th>\n",
       "      <th>once-in-a-lifetime</th>\n",
       "      <th>oversimplification</th>\n",
       "      <th>unconstitutionally</th>\n",
       "      <th>extraterritoriality</th>\n",
       "      <th>mischaracterization</th>\n",
       "      <th>unconstitutionality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>1613</td>\n",
       "      <td>207</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>340</td>\n",
       "      <td>273</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "      <td>1867</td>\n",
       "      <td>281</td>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>284</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "      <td>2324</td>\n",
       "      <td>212</td>\n",
       "      <td>1635</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>429</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>1646</td>\n",
       "      <td>204</td>\n",
       "      <td>944</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "      <td>266</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "      <td>1763</td>\n",
       "      <td>227</td>\n",
       "      <td>1115</td>\n",
       "      <td>0</td>\n",
       "      <td>410</td>\n",
       "      <td>263</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>1890</td>\n",
       "      <td>211</td>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09</th>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>2128</td>\n",
       "      <td>275</td>\n",
       "      <td>1247</td>\n",
       "      <td>0</td>\n",
       "      <td>461</td>\n",
       "      <td>321</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>1723</td>\n",
       "      <td>235</td>\n",
       "      <td>1057</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>1852</td>\n",
       "      <td>188</td>\n",
       "      <td>1090</td>\n",
       "      <td>0</td>\n",
       "      <td>402</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07</th>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>1633</td>\n",
       "      <td>196</td>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>1863</td>\n",
       "      <td>192</td>\n",
       "      <td>1194</td>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>17</td>\n",
       "      <td>1757</td>\n",
       "      <td>231</td>\n",
       "      <td>1111</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>285</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ha  no  ass  axe   bad  ban   big  cry  cut  die  ...  \\\n",
       "2019-02   84   0   53   19  1613  207  1070    0  340  273  ...   \n",
       "2019-03   66   0   72   11  1867  281  1151    0  403  284  ...   \n",
       "2019-12   90   0   89   19  2324  212  1635    0  582  429  ...   \n",
       "2019-06   54   0   58   11  1646  204   944    0  442  266  ...   \n",
       "2019-08   56   0   63   13  1763  227  1115    0  410  263  ...   \n",
       "2019-11   59   0   53   11  1890  211  1284    0  578  265  ...   \n",
       "2019-09  580   0   56   13  2128  275  1247    0  461  321  ...   \n",
       "2019-04   75   0   60    9  1723  235  1057    0  345  267  ...   \n",
       "2019-01   74   0   57   14  1852  188  1090    0  402  242  ...   \n",
       "2019-07  117   0   68   10  1633  196   998    0  403  253  ...   \n",
       "2019-05   89   0   51   16  1863  192  1194    0  382  264  ...   \n",
       "2019-10   51   0   68   17  1757  231  1111    0  365  285  ...   \n",
       "\n",
       "         misclassifications  misinterpretations  misrepresentations  \\\n",
       "2019-02                   0                   0                   1   \n",
       "2019-03                   0                   1                   4   \n",
       "2019-12                   0                   0                   2   \n",
       "2019-06                   0                   1                   2   \n",
       "2019-08                   0                   1                   0   \n",
       "2019-11                   0                   2                   2   \n",
       "2019-09                   0                   5                   1   \n",
       "2019-04                   0                   0                   3   \n",
       "2019-01                   0                   1                   2   \n",
       "2019-07                   0                   1                   1   \n",
       "2019-05                   0                   1                   1   \n",
       "2019-10                   0                   0                   1   \n",
       "\n",
       "         multifunctionality  once-in-a-lifetime  oversimplification  \\\n",
       "2019-02                   0                   0                   7   \n",
       "2019-03                   0                   0                   5   \n",
       "2019-12                   0                   0                   9   \n",
       "2019-06                   0                   0                  10   \n",
       "2019-08                   0                   0                   7   \n",
       "2019-11                   0                   0                   6   \n",
       "2019-09                   0                   0                   6   \n",
       "2019-04                   0                   0                  11   \n",
       "2019-01                   0                   0                   6   \n",
       "2019-07                   0                   0                   7   \n",
       "2019-05                   0                   0                  15   \n",
       "2019-10                   0                   0                   7   \n",
       "\n",
       "         unconstitutionally  extraterritoriality  mischaracterization  \\\n",
       "2019-02                   0                    0                    1   \n",
       "2019-03                   0                    2                    0   \n",
       "2019-12                   1                    0                    0   \n",
       "2019-06                   1                    0                    1   \n",
       "2019-08                   2                    0                    1   \n",
       "2019-11                   0                    0                    0   \n",
       "2019-09                   2                    0                    0   \n",
       "2019-04                   0                    0                    3   \n",
       "2019-01                   0                    0                    1   \n",
       "2019-07                   2                    1                    0   \n",
       "2019-05                   0                    0                    2   \n",
       "2019-10                   0                    0                    1   \n",
       "\n",
       "         unconstitutionality  \n",
       "2019-02                    0  \n",
       "2019-03                    0  \n",
       "2019-12                    0  \n",
       "2019-06                    0  \n",
       "2019-08                    0  \n",
       "2019-11                    0  \n",
       "2019-09                    2  \n",
       "2019-04                    0  \n",
       "2019-01                    1  \n",
       "2019-07                    0  \n",
       "2019-05                    0  \n",
       "2019-10                    0  \n",
       "\n",
       "[12 rows x 7552 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = word_frequency_pipe.fit_transform(months_of_text)\n",
    "result = pd.DataFrame(matrix.toarray(), columns= word_frequency_pipe['count_vec'].get_feature_names(), index=index)\n",
    "result.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/reddit2019_tonal_wordcounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_results(pipeline_transformer: Pipeline, text: list) -> pd.DataFrame:\n",
    "    matrix = pipeline_transformer.fit_transform(text)\n",
    "    \n",
    "    return pd.DataFrame(matrix.toarray(), columns= pipeline_transformer['count_vec'].get_feature_names(), index=index)\n",
    "\n",
    "#word_frequency_result = get_results(word_frequency_pipe, months_of_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ha</th>\n",
       "      <th>no</th>\n",
       "      <th>ass</th>\n",
       "      <th>axe</th>\n",
       "      <th>bad</th>\n",
       "      <th>ban</th>\n",
       "      <th>big</th>\n",
       "      <th>cry</th>\n",
       "      <th>cut</th>\n",
       "      <th>die</th>\n",
       "      <th>...</th>\n",
       "      <th>misclassifications</th>\n",
       "      <th>misinterpretations</th>\n",
       "      <th>misrepresentations</th>\n",
       "      <th>multifunctionality</th>\n",
       "      <th>once-in-a-lifetime</th>\n",
       "      <th>oversimplification</th>\n",
       "      <th>unconstitutionally</th>\n",
       "      <th>extraterritoriality</th>\n",
       "      <th>mischaracterization</th>\n",
       "      <th>unconstitutionality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02</th>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.061424</td>\n",
       "      <td>0.007883</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>0.010396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03</th>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.056476</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.034817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014352</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06</th>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.037638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08</th>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.064344</td>\n",
       "      <td>0.008285</td>\n",
       "      <td>0.040694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ha   no       ass       axe       bad       ban       big  cry  \\\n",
       "2019-02  0.003199  0.0  0.002018  0.000724  0.061424  0.007883  0.040746  0.0   \n",
       "2019-03  0.001996  0.0  0.002178  0.000333  0.056476  0.008500  0.034817  0.0   \n",
       "2019-12  0.002219  0.0  0.002195  0.000469  0.057308  0.005228  0.040318  0.0   \n",
       "2019-06  0.002153  0.0  0.002312  0.000439  0.065627  0.008134  0.037638  0.0   \n",
       "2019-08  0.002044  0.0  0.002299  0.000474  0.064344  0.008285  0.040694  0.0   \n",
       "\n",
       "              cut       die  ...  misclassifications  misinterpretations  \\\n",
       "2019-02  0.012947  0.010396  ...                 0.0            0.000000   \n",
       "2019-03  0.012191  0.008591  ...                 0.0            0.000041   \n",
       "2019-12  0.014352  0.010579  ...                 0.0            0.000000   \n",
       "2019-06  0.017623  0.010606  ...                 0.0            0.000055   \n",
       "2019-08  0.014964  0.009599  ...                 0.0            0.000050   \n",
       "\n",
       "         misrepresentations  multifunctionality  once-in-a-lifetime  \\\n",
       "2019-02            0.000041                 0.0                 0.0   \n",
       "2019-03            0.000131                 0.0                 0.0   \n",
       "2019-12            0.000053                 0.0                 0.0   \n",
       "2019-06            0.000086                 0.0                 0.0   \n",
       "2019-08            0.000000                 0.0                 0.0   \n",
       "\n",
       "         oversimplification  unconstitutionally  extraterritoriality  \\\n",
       "2019-02            0.000267            0.000000             0.000000   \n",
       "2019-03            0.000151            0.000000             0.000149   \n",
       "2019-12            0.000222            0.000044             0.000000   \n",
       "2019-06            0.000399            0.000071             0.000000   \n",
       "2019-08            0.000255            0.000129             0.000000   \n",
       "\n",
       "         mischaracterization  unconstitutionality  \n",
       "2019-02             0.000057                  0.0  \n",
       "2019-03             0.000000                  0.0  \n",
       "2019-12             0.000000                  0.0  \n",
       "2019-06             0.000059                  0.0  \n",
       "2019-08             0.000054                  0.0  \n",
       "\n",
       "[5 rows x 7552 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_result = get_results(word_tfidf_pipe, months_of_text)\n",
    "tfidf_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_result.to_csv('data/reddit2019_tonal_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01</th>\n",
       "      <td>100.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02</th>\n",
       "      <td>100.7097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03</th>\n",
       "      <td>101.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04</th>\n",
       "      <td>101.2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05</th>\n",
       "      <td>101.3725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         confidence\n",
       "date               \n",
       "2014-01    100.3960\n",
       "2014-02    100.7097\n",
       "2014-03    101.0020\n",
       "2014-04    101.2360\n",
       "2014-05    101.3725"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the outcome variable and preprocess\n",
    "uk_confidence = import_uk_confidence()\n",
    "\n",
    "uk_confidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = uk_confidence.loc[tfidf_result.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.93133883, 13.63588972, 14.18392673, 14.72960666, 14.69030981,\n",
       "        8.53976176,  7.78326387,  4.23331689,  4.23441684,  4.07295309])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not sure on how to correlate a matrix with against a vector, is this right? \n",
    "# Values don't look right!\n",
    "\n",
    "correlation = np.correlate(tfidf_result.values.flatten(), y.values.flatten())\n",
    "correlation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_quarterly(row):\n",
    "    if row[\"Title\"].endswith(\"Q1\"):\n",
    "        return row[\"Title\"].replace(\" Q1\", \"-01-01\")\n",
    "    elif row[\"Title\"].endswith(\"Q2\"):\n",
    "        return row[\"Title\"].replace(\" Q2\", \"-04-01\")\n",
    "    elif row[\"Title\"].endswith(\"Q3\"):\n",
    "        return row[\"Title\"].replace(\" Q3\", \"-07-01\")\n",
    "    elif row[\"Title\"].endswith(\"Q4\"):\n",
    "        return row[\"Title\"].replace(\" Q4\", \"-10-01\")\n",
    "\n",
    "def import_household_savings() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns UK household savings ratios\n",
    "    \"\"\"\n",
    "    household_savings_df = pd.read_csv(\"data/household_savings_ratio.csv\")\n",
    "    household_savings_df[\"quarterly_data\"] = [\n",
    "        True if \"Q\" in x else False for x in household_savings_df[\"Title\"]\n",
    "    ]\n",
    "    household_savings_df = household_savings_df[\n",
    "        household_savings_df[\"quarterly_data\"] == True\n",
    "    ]\n",
    "    household_savings_df[\"date\"] = pd.to_datetime(\n",
    "        household_savings_df.apply(replace_quarterly, axis=1)\n",
    "    ).dt.to_period(\"M\")\n",
    "    household_savings_df = household_savings_df.set_index(\"date\").resample(\"M\").ffill()\n",
    "    household_savings_df = household_savings_df.rename(\n",
    "        {\n",
    "            \"Households (S.14): Households' saving ratio (per cent): Current price: £m: SA\": \"savings_ratio\"\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    return household_savings_df[[\"savings_ratio\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "household_savings = import_household_savings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_savings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_uk_confidence():\n",
    "    all_confidence = pd.read_csv('data/consumer_confidence_index.csv',\n",
    "                               usecols=['TIME', 'Value', 'LOCATION'])\n",
    "\n",
    "    uk_confidence = all_confidence.loc[all_confidence.LOCATION == \"GBR\"]\n",
    "\n",
    "    assert all(pd.value_counts(uk_confidence.TIME) == 1), \"duplicate entries for the same time period\"\n",
    "\n",
    "    date = pd.to_datetime(uk_confidence.TIME, format=\"%Y-%m\")\n",
    "\n",
    "    # clean dataframe:\n",
    "    df = pd.DataFrame({'date': date, 'confidence': uk_confidence.Value})\n",
    "    df['date'] = df['date'].dt.to_period(\"M\")\n",
    "    df = df.set_index(['date'], drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([household_savings, uk_confidence], axis=1).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.concat([household_savings, uk_confidence], axis=1).dropna()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
